{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity Analysis of Dialogflow CX Pages and Ad Hoc Input Data\n",
    "In this notebook, we will show you how to uses NLU sentence embeddings to determine how similar different utterances are. We use this information to perform the following analyses:\n",
    "\n",
    "* Find similar training phrases in different intents that will cause confusion for the NLU model.\n",
    "* Identify the most similar training phrases for a user-supplied set of utterances. This will explain where incorrect predictions are coming from on an eval set.\n",
    "* Identify clusters of utterances that are unlike any of the phrases in the training data. This can be used to search through utterances that produced NO_MATCH in the logs and identify missing intents/training phrases.\n",
    "\n",
    "## Prerequisites\n",
    "- Ensure you have a GCP Service Account key with the Dialogflow API Admin privileges assigned to it.\n",
    "\n",
    "## NOTE!\n",
    "_This colab notebook was intended to run on [Google Colab](https://colab.sandbox.google.com/) infrastrucutre due to the `scann` library dependency._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't already, make sure you install the `dfcx-scrapi` library\n",
    "\n",
    "!pip install dfcx-scrapi\n",
    "!pip install scann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "During import, Colab will ask you to auth with your Google credentials.  \n",
    "The creds are used to access Google Sheets where your training data lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "\n",
    "from dfcx_scrapi.tools.nlu_util import KonaEmbeddingModel, SheetsLoader, NaturalLanguageUnderstandingUtil\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = default()\n",
    "\n",
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs\n",
    "In the next section, we will collect runtime variables needed to execute this notebook.   \n",
    "This should be the only cell of the notebook you need to edit in order for this notebook to run.\n",
    "\n",
    "Getting an the training phrsaes data from your existing DFCX agent requires the following information:\n",
    "- `creds_path`, path to your service account credentials file.\n",
    "- `agent_id`, which is your GCP agent ID.\n",
    "- `flow_display_name`, the Display Name of the Flow to use\n",
    "- `page_display_name`, the Display Name of the Page to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_path = '<PATH_TO_YOUR_CREDS_FILE>'\n",
    "agent_id = '<YOUR_AGENT_ID>'\n",
    "flow_display_name = '<YOUR_DFCX_FLOW_DISPLAY_NAME>'\n",
    "page_display_name = '<YOUR_DFCX_PAGE_DISPLAY_NAME>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Agent\n",
    "First, we will instantiate our `embedder` by loading in Agent, Flow, and Page information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = NaturalLanguageUnderstandingUtil(agent_id, flow_display_name, page_display_name, creds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: Find conflicting training phrases\n",
    "\n",
    "This analysis identifies pairs of training phrases in different intents that are confusing for the NLU model.\n",
    "\n",
    "We recommend reviewing all training phrases with a similarity above 0.9. In most cases the conflicts should be resolved by deleting one of the training phrases.\n",
    "\n",
    "No additional inputs are needed to run this analysis. Just run this cell after loading the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df = embedder.find_similar_training_phrases_in_different_intents()\n",
    "similar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35360e9",
   "metadata": {},
   "source": [
    "# Analysis 2: Find the most similar training phrases for a set of utterances\n",
    "\n",
    "This analysis finds the most similar training phrases in the agent data for a set of utterances. \n",
    "\n",
    "Example use cases:\n",
    "* Identifying why utterances in an eval set were classified as some intent.\n",
    "* Looking for any similar training phrases for a list of utterances from the logs.\n",
    "\n",
    "We read the utterances from a Google Sheet. That sheet must be shared with your service account creds file that was provided in this notebook.\n",
    "\n",
    "We'll collect the following input arguments to access your Google Sheet:\n",
    "* `sheet_name`, the display name of your Google Sheet\n",
    "* `worksheet_name`, the display name of the Worksheet or tab where your data lives\n",
    "* `utterance_column_name`, the name of the column where your Utterance data lives\n",
    "\n",
    "Input data can be a single column of utterances like:\n",
    "|  utterances   |\n",
    "| --- | \n",
    "| new york |\n",
    "| big apple |\n",
    "| I like traveling to nyc |\n",
    "| some people call it the big apple |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa387eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = \"<YOUR_GOOGLE_SHEET_NAME>\"\n",
    "worksheet_name = \"<YOUR_GOOGLE_SHEET_WORKSHEET_NAME>\"\n",
    "utterance_column_name = \"utterances\"\n",
    "\n",
    "sheet_loader = SheetsLoader(creds_path)\n",
    "utterances = sheet_loader.load_column_from_sheet(sheet_name, worksheet_name, utterance_column_name).astype(str)\n",
    "embedder.find_similar_phrases(utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cda60e",
   "metadata": {},
   "source": [
    "# Analysis 3: Find clusters of utterances that don't match training phrases\n",
    "\n",
    "This analysis finds clusters of utterances that don't match any training phrases. Example use cases:\n",
    "\n",
    "* Running on a set of utterances that were labeled NO_MATCH in logs. To fix any entries that show up, consider:\n",
    "  * Adding the identified utterances as training phrases in an existing intent (the most similar intent will be displayed).\n",
    "  * Adding new intents.\n",
    "  * Enabling inactive intents.\n",
    "* Running on a set of eval/log utterances to identify regions where an intent is lacking training phrases.\n",
    "  * To fix any entries that show up, we recommend adding the identified utterances as training phrases.\n",
    "\n",
    "We read the utterances from a Google Sheet. That sheet must be shared with your service account creds file that was provided in this notebook.\n",
    "\n",
    "We'll collect the following input arguments to access your Google Sheet:\n",
    "* `sheet_name`, the display name of your Google Sheet\n",
    "* `worksheet_name`, the display name of the Worksheet or tab where your data lives\n",
    "* `utterance_column_name`, the name of the column where your Utterance data lives\n",
    "\n",
    "Input data can be a single column of utterances like:\n",
    "|  no_match_utterances   |\n",
    "| --- | \n",
    "| new york |\n",
    "| big apple |\n",
    "| I like traveling to nyc |\n",
    "| some people call it the big apple |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fe87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = \"<YOUR_GOOGLE_SHEET_NAME>\"\n",
    "worksheet_name = \"<YOUR_GOOGLE_SHEET_WORKSHEET_NAME>\"\n",
    "utterance_column_name = \"no_match_utterances\"\n",
    "\n",
    "sheet_loader = SheetsLoader(creds_path)\n",
    "utterances = sheet_loader.load_column_from_sheet(sheet_name, \n",
    "                                                 worksheet_name, \n",
    "                                                 utterance_column_name).astype(str)\n",
    "embedder.find_new_groups(utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts and Wrap-Up\n",
    "In this notebook, we've shown you how to uses NLU sentence embeddings to determine how similar different utterances are.\n",
    "\n",
    "For further instruction, please contact: [cgibson6279](https://github.com/cgibson6279)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
