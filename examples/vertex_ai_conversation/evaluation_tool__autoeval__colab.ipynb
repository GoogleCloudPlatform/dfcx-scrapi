{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZzOhx3sNWR"
      },
      "source": [
        "# Vertex AI Conversation - Evaluation Tool\n",
        "\n",
        "This tool requires user's input in several steps. Please run the cells one by one (Shift+Enter) to ensure all the steps are successfully completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afvsuux0zaWZ"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "0U8xQwhKrOUq"
      },
      "outputs": [],
      "source": [
        "# @markdown `install packages`\n",
        "!pip install dfcx-scrapi --quiet\n",
        "\n",
        "# workaround until vertexai import is fixed\n",
        "!pip uninstall bigframes -y --quiet\n",
        "!pip install bigframes==0.26.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import dataclasses\n",
        "import vertexai\n",
        "import pandas as pd\n",
        "\n",
        "from dfcx_scrapi.tools.datastore_scraper import DataStoreScraper, load_spreadsheet\n",
        "from dfcx_scrapi.tools.datastore_evaluator import DataStoreEvaluator, EvaluationVisualizer, EvaluationResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EgoRHwBJqJ0r"
      },
      "outputs": [],
      "source": [
        "# @markdown `authenticate`\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.auth import default\n",
        "    from google.colab import auth\n",
        "    from google.colab import files\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    credentials, _ = default()\n",
        "else:\n",
        "    # Otherwise, attempt to discover local credentials as described in\n",
        "    # https://cloud.google.com/docs/authentication/application-default-credentials\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srTefHVmJ3U"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QbzhGINr5IkG"
      },
      "outputs": [],
      "source": [
        "# @markdown `initialize vertex ai`\n",
        "\n",
        "# @markdown > The project selected will be billed for calculating evaluation\n",
        "# @markdown metrics that require large language models. It should have the\n",
        "# @markdown [Vertex AI API](https://cloud.google.com/vertex-ai/docs/featurestore/setup)\n",
        "# @markdown enabled. The LLM-based metrics use PaLM 2 for Text (Text Bison).\n",
        "# @markdown For pricing information see this [page](https://cloud.google.com/vertex-ai/generative-ai/pricing).\n",
        "\n",
        "vertex_ai_project_id = \"\"  # @param{type: 'string'}\n",
        "vertex_ai_location = \"\"  # @param{type: 'string'}\n",
        "\n",
        "vertexai.init(\n",
        "    project=vertex_ai_project_id,\n",
        "    location=vertex_ai_location,\n",
        "    credentials=credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rZB_riHj13Mo"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to initialize Dialogflow CX agent scraper`\n",
        "# @markdown > This cell initializes the agent with the provided parameters.\n",
        "# @markdown `project_id`, `location` and `agent_id` can be defined through one\n",
        "# @markdown of the following options, while `language_code` must be defined\n",
        "# @markdown in either case. The parameters for a given agent can be found in\n",
        "# @markdown the DialogflowCX console url:\n",
        "# @markdown `https://dialogflow.cloud.google.com/cx/projects/`**`{project_id}`**`/locations/`**`{location}`**`/agents/`**`{agent_id}`**`/intents`\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Option 1. - Provide agent_id directly\n",
        "# @markdown Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>`\n",
        "\n",
        "agent_id = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Option 2. - Parse agent_id from url\n",
        "# @markdown > **NOTE** : if `agent_url` is provided then it has precedence over\n",
        "# @markdown directly provided agent_id.\n",
        "\n",
        "agent_url = \"\" # @param {type: \"string\"}\n",
        "\n",
        "if agent_url:\n",
        "  scraper = DataStoreScraper.from_url(agent_url=agent_url, creds=credentials)\n",
        "else:\n",
        "  scraper = DataStoreScraper(agent_id=agent_id, creds=credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z50iGm_rTC49"
      },
      "outputs": [],
      "source": [
        "# test agent on a single query\n",
        "response = scraper.scrape_detect_intent(query=\"who is the ceo?\")\n",
        "print(json.dumps(dataclasses.asdict(response), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFExd6nTmLYr"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "The queryset must be in a tabular format that has to contain the following columns:\n",
        "- `conversation_id` _(unique identifier of a conversation, which must be the same for each row that are part of the same conversation)_\n",
        "- `turn_index` _(index of the query - expected answer pair within a conversation)_\n",
        "- `query` _(the input question)_\n",
        "- `expected_answer` _(the ideal or ground truth answer)_\n",
        "- `expected_uri` _(the webpage url or more generally the uri that contains the answer to `query`)_.\n",
        "- `user_metadata` _(optional user metadata passed to datastore route with user query. Can be one of [str, dict])_\n",
        "- `parameters` _(optional session parameters to include with the user query. Can be one of [str, dict])_\n",
        "\n",
        "In addition to the required columns the RougeL metric can also use the following optional column:\n",
        "\n",
        "- `golden_snippet` _(the extracted document snippet or segment that contains the `expected_answer`)_\n",
        "\n",
        "An example for the queryset can be seen in this table:\n",
        "\n",
        "| conversation_id | turn_index | query | expected_answer | expected_uri | user_metadata | parameters\n",
        "| --- | --- | --- | --- | --- | --- | --- |\n",
        "| 0 | 1 | What is the capital of France? | Capital of France is Paris. | exampleurl.com/france | None | None |\n",
        "| 0 | 2 | How many people live there? | 2.1 million people live in Paris. | exampleurl.com/paris | {\"some_end_user_key\": \"some_value\"} | {\"param1\": 1, \"param2\": \"some_string\"} |\n",
        "| 1 | 1 | What is the color of the sky? | It is blue. | exampleurl.com/common | None | None |\n",
        "| 2 | 1 | How many legs does an octopus have? | It has 8 limbs. | exampleurl.com/octopus | None | None |\n",
        "\n",
        "---\n",
        "\n",
        "Choose one of the following 3 options to load the queryset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InymjofmW5X"
      },
      "source": [
        "### Option 1. - Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qwhOvOSOmnJ4"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to load data manually`\n",
        "INPUT_SCHEMA_REQUIRED_COLUMNS = [\n",
        "    \"conversation_id\", \"turn_index\", \"query\", \"expected_answer\", \"expected_uri\", \"user_metadata\", \"parameters\"\n",
        "]\n",
        "\n",
        "sample_df = pd.DataFrame(columns=INPUT_SCHEMA_REQUIRED_COLUMNS)\n",
        "\n",
        "sample_df.loc[0] = [\"0\", 1 ,\"Who are you?\", \"I am an assistant\", \"www.google.com\", None, None]\n",
        "sample_df.loc[1] = [\"1\", 1 ,\"Who is the CEO?\", \"The CEO is Matt Reinjes.\", \"www.yeti.com\", {\"some_end_user_key\": \"some_value\"}, {\"param1\": 1, \"param2\": \"some_string\"}]\n",
        "sample_df.loc[2] = [\"1\", 2, \"How much?\", \"The Basic plan costs 20$/month\", \"www.google.com\", None, None]\n",
        "queryset = sample_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WlIWM21mUK_"
      },
      "source": [
        "---\n",
        "### Option 2. - From local .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "47JSfzvfaVhd"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to load the queryset from a .csv file in the filesystem`\n",
        "\n",
        "csv_path = \"\"  # @param{type: 'string'}\n",
        "\n",
        "queryset = pd.read_csv(csv_path)\n",
        "queryset = queryset.fillna(\"\")\n",
        "\n",
        "if \"user_metadata\" in queryset.columns:\n",
        "  queryset = queryset.assign(\n",
        "      user_metadata=queryset[\"user_metadata\"].apply(lambda p:p if p != \"\" else None)\n",
        "    )\n",
        "else:\n",
        "  queryset = queryset.assign(user_metadata=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfEOv6cTmRDB"
      },
      "source": [
        "---\n",
        "### Option 3. - From Google Sheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A2tS-n1PRHeC"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to load the queryset from Google Sheets`\n",
        "\n",
        "sheet_url = \"\" # @param {type: \"string\"}\n",
        "worksheet_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown > **NOTE**: if `worksheet_name` is not provided then `Sheet1` is used\n",
        "# @markdown by default.\n",
        "\n",
        "_worksheet_name = worksheet_name if worksheet_name else \"Sheet1\"\n",
        "\n",
        "queryset = load_spreadsheet(sheet_url, _worksheet_name, credentials)\n",
        "\n",
        "if \"user_metadata\" in queryset.columns:\n",
        "  queryset = queryset.assign(\n",
        "      user_metadata=queryset[\"user_metadata\"].apply(lambda p:p if p != \"\" else None)\n",
        "    )\n",
        "else:\n",
        "  queryset = queryset.assign(user_metadata=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_TZfWQmjtv"
      },
      "source": [
        "## Scraping Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7phOx3LWHUfm"
      },
      "outputs": [],
      "source": [
        "# run scraping\n",
        "scrape_result = scraper.run(queryset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6OKOpImcJg"
      },
      "source": [
        "## Metric Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at5ss5szde8o"
      },
      "source": [
        "Select the metrics that should be computed during evaluation.\n",
        "\n",
        "> **NOTE** : Remember to rerun the cell below (Shift+Enter) after clicking the checkbox of the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UaX42vFgwwYA"
      },
      "outputs": [],
      "source": [
        "URL_MATCH = True # @param {type: \"boolean\"}\n",
        "# @markdown url match metric computes the following boolean type columns:\n",
        "# @markdown - `cited_url_match@1` - is `expected_url` same as the first link returned by Vertex AI Conversation\n",
        "# @markdown - `cited_url_match` - is `expected_url` part of the links returned by Vertex AI Conversation\n",
        "# @markdown - `search_url_match` - is `expected_url` part of the search results that are shown to generative model in Vertex AI Conversation\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "ROUGEL = True # @param {type: \"boolean\"}\n",
        "# @markdown rougeL metric computes a score between `[0, 1]` (higher is better) based on\n",
        "# @markdown the longest common word subsequence between different targets and predictions:\n",
        "# @markdown\n",
        "# @markdown _(`cited_search_results` are the search snippets which were cited by answer generator llm)_\n",
        "# @markdown - `rougeL_generative` - Compares `expected_answer` to `answer_text`.\n",
        "# @markdown - `rougeL_extractive` - (only computed if\n",
        "# @markdown `golden_snippet` is part of the dataset) Compares `golden_snippet` to `answer_snippets[0]`\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "ANSWER_CORRECTNESS = True # @param {type: \"boolean\"}\n",
        "# @markdown LLM-based autoeval metric that compares `expected_answer` to `answer_text`. The metric\n",
        "# @markdown makes approximately 5-10 LLM calls for each for each row of the dataset depening on `expected_answer` and `answer_text` length.\n",
        "# @markdown It returns 3 output columns containing scores between `[0, 1]` (higher is better):\n",
        "# @markdown - `answer_correctness_recall` - How well does `answer_text`'s information content cover `expected_answer`.\n",
        "# @markdown - `answer_correctness_precision` - How much of `answer_text`'s information content is required based on `expected_answer`.\n",
        "# @markdown - `answer_correctness_f1` - The harmonic mean of recall and precision.\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "FAITHFULNESS = True # @param {type: \"boolean\"}\n",
        "# @markdown LLM-based autoeval metric that provides a score between `[0, 1]` (higher is better)\n",
        "# @markdown with regard to how well `answer_text` is attributed to `search_snippets`. It makes approximately 5\n",
        "# @markdown LLM calls for each row of the dataset depending on the length of `answer_text`.\n",
        "# @markdown - `faithfulness_gmean`\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "CONTEXT_RECALL = True # @param {type: \"boolean\"}\n",
        "# @markdown LLM-based autoeval metric that provides a score between `[0, 1]` (higher is better)\n",
        "# @markdown for how well the `expected_answer` is attributed to `search_snippets`. In other words this metric\n",
        "# @markdown scores the quality of search by measuring how well the `expected_answer` can be generated from the `search_snippets`.\n",
        "# @markdown It makes approximately 5 LLM calls for each row of the dataset depending on the length of `expected_answer`.\n",
        "# @markdown - `context_recall_gmean`\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "\n",
        "metrics = []\n",
        "\n",
        "if URL_MATCH: metrics.append(\"url_match\")\n",
        "if ROUGEL: metrics.append(\"rougeL\")\n",
        "if ANSWER_CORRECTNESS: metrics.append(\"answer_correctness\")\n",
        "if FAITHFULNESS: metrics.append(\"faithfulness\")\n",
        "if CONTEXT_RECALL: metrics.append(\"context_recall\")\n",
        "\n",
        "evaluator = DataStoreEvaluator(metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69h01DPmrMoA"
      },
      "source": [
        "## Computing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "9NeMsvykHb0E"
      },
      "outputs": [],
      "source": [
        "# @markdown `evaluation results`\n",
        "evaluation_result = evaluator.run(scrape_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAZt4TG3Pnwe"
      },
      "source": [
        "## Export results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjWYe58P25A"
      },
      "source": [
        "### Option 1. - Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mLNaJ8S-RC4O"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to display evaluation results`\n",
        "Number_of_rows = 3 # @param {type: \"integer\"}\n",
        "\n",
        "\n",
        "results=evaluation_result.display_on_screen()\n",
        "results.head(Number_of_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfxfaXQdQF-p"
      },
      "source": [
        "### Option 2. - To local.csv and download to your system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z4yFlm97rIRp"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to export evaluation results into Google Sheets`\n",
        "\n",
        "FILE_NAME = \"evaluation_results.csv\" # @param {type: \"string\"}\n",
        "\n",
        "filepath = evaluation_result.export_to_csv(FILE_NAME)\n",
        "\n",
        "# Prompt user to download the file\n",
        "print(f\"CSV file created at: {filepath}\")\n",
        "print(\"Would you like to download the file? (y/n)\")\n",
        "user_choice = input().lower()\n",
        "\n",
        "if user_choice == \"y\":\n",
        "  # Download the file using Colab's download feature\n",
        "  files.download(filepath)\n",
        "  print(\"File downloaded successfully!\")\n",
        "else:\n",
        "  print(\"Download skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsZmpIBpQIu9"
      },
      "source": [
        "### Option 3. - To Google Sheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YzjPmsPVUaJt"
      },
      "outputs": [],
      "source": [
        "# @markdown `run this cell to export evaluation results into Google Sheets`\n",
        "\n",
        "FOLDER_NAME = \"result\" # @param {type: \"string\"}\n",
        "CHUNK_SIZE = 50 # @param {type: \"number\"}\n",
        "WITH_TIMESTAMP = True # @param {type: \"boolean\"}\n",
        "\n",
        "_folder_name = (\n",
        "    f\"{FOLDER_NAME}_{evaluation_result.timestamp}\"\n",
        "    if WITH_TIMESTAMP else\n",
        "    FOLDER_NAME\n",
        ")\n",
        "\n",
        "folder_url = evaluation_result.export(_folder_name, CHUNK_SIZE, credentials)\n",
        "print(f\"Exported results to folder: {folder_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8WDndEnpj82"
      },
      "source": [
        "### Option 4. - To Bigquery\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CwUj_bosp3Mv"
      },
      "outputs": [],
      "source": [
        "BQ_PROJECT_ID=\"\" # @param {type: \"string\"}\n",
        "BQ_DATASET_ID=\"\" # @param {type: \"string\"}\n",
        "BQ_TABLE_NAME =\"\" # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "filepath = evaluation_result.export_to_bigquery(BQ_PROJECT_ID,BQ_DATASET_ID,BQ_TABLE_NAME,credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7ddvNEUbal"
      },
      "source": [
        "## Result Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kARLoOJYBJ0e"
      },
      "outputs": [],
      "source": [
        "# @markdown `Folder url`\n",
        "FOLDER_URLS = [\n",
        "    folder_url, # latest evaluation\n",
        "    # add previous evaluations e.g: https://drive.google.com/drive/folders/<id>\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "umJE__jOWTQu"
      },
      "outputs": [],
      "source": [
        "# @markdown `define evaluation visualizer`\n",
        "\n",
        "results = []\n",
        "for folder_url in FOLDER_URLS:\n",
        "  er = EvaluationResult()\n",
        "  er.load(folder_url, credentials)\n",
        "  results.append(er)\n",
        "\n",
        "evaluation_visualizer = EvaluationVisualizer(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LqObjkW28pNa"
      },
      "outputs": [],
      "source": [
        "# @markdown `radar plot of autoeval metrics`\n",
        "from dfcx_scrapi.tools.metrics import StatementBasedBundledMetric, RougeL\n",
        "\n",
        "evaluation_visualizer.radar_plot(StatementBasedBundledMetric.COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "H-wg1FTiToA5"
      },
      "outputs": [],
      "source": [
        "# @markdown `response type distribution`\n",
        "\n",
        "evaluation_visualizer.count_barplot(\"response_type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6nc7WQKVY1jp"
      },
      "outputs": [],
      "source": [
        "# @markdown `average RougeL`\n",
        "\n",
        "evaluation_visualizer.mean_barplot(column_names=RougeL.COLUMNS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
